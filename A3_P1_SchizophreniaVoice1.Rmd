---
title: "Assignment2_Part1_VoiceInSchizophrenia"
author: "Riccardo Fusaroli"
date: "July 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 2 - Part 1 - Assessing voice in schizophrenia

Schizophrenia has been associated with "inappropriate" voice, sometimes monotone, sometimes croaky. A few studies indicate that pitch might be an index of schizophrenia. However, an ongoing meta-analysis of the literature (which you will have a go at in the last assignment) indicates that pitch mean and standard deviation are only weak indicators of diagnosis. Can we do better with our new fancy complex skills?

The corpus you are asked to analyse is a set of voice recordings from people with schizophrenia (just after first diagnosis) and 1-1 matched controls (on gender, age, education). Each participant watched 10 videos of triangles moving across the screen and had to describe them (so you have circa 10 recordings per person). I have already extracted the pitch once every 10 milliseconds and you will have to use this data to assess differences in the voice.

N.B. Question to be answered via email to Celine: can you characterize voice in schizophrenia as acoustically different? Report the methods you used to answer this question and the results from the analyses. Add a couple of lines trying to interpret the results (make sense of the difference). E.g. People with schizophrenia tend to have high-pitched voice, and present bigger swings in their prosody than controls. Add a couple of lines describing limitations of the data/analyses if any is relevant.

N.B. There are looots of files to be dealt with. Probably too many for your computer. This is a challenge for you. Some (complementary) possible strategies: You can select a subset of files only (and you have to justify your choice). You can learn how to use the apply() or map() functions. You can coordinate with classmates.

1. In the course of this assignment you have to first select one datafile and figure out how to:

- Extract "standard" descriptors of pitch: Mean, standard deviation, range
- Extract less "standard" descriptors of pitch you can think of (e.g. median, iqr, mean absoluted deviation, coefficient of variation)
- Extract "complex" descriptors: recurrence quantification analysis
```{r}
p_load(tidyverse, crqa, lsr, stringr, lme4, lmerTest)

#loading files
arti = read.csv("Articulation.txt", sep=",")
demo = read.delim("DemoData.txt")
pitch1 = read.delim("Pitch/Pitch/Study1D0S101T2_f0.txt")

str_extract("Study1D0S101T2_f0.txt", "T(\\d+)")
?str_extract
summary(pitch1)
#mean
meanp1 = mean(pitch1$f0)
#sd
sdp1 = sd(pitch1$f0)

#range
diffrange = max(pitch1$f0) - min(pitch1$f0)
#median
medianp1 = median(pitch1$f0)
#inter-quartile range
iqrp1 = IQR(pitch1$f0)

#mean absolute deviation
madp1 = aad(pitch1$f0)
#coefficient of variation (sd/mean)
coefvarp1 = sdp1/meanp1

#RQAAA
par = list(lgM =  50, steps = seq(1, 6, 1),  radiusspan = 100,  radiussample = 40, normalize = 0,  rescale = 0,  mindiagline = 2,  
minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE,  fnnpercent = 10,  typeami = "mindip")

ans = optimizeParam(pitch1$f0, pitch1$f0, par, min.rec = 3.5, max.rec = 4.5)
ans

Results=crqa(pitch1$f0, pitch1$f0, delay=ans$delay, embed=ans$emddim, radius=ans$radius,normalize=0,rescale=0,mindiagline = 2,minvertline = 2) 

Results

#Creating plot
RP = Results$RP
RP = matrix(as.numeric(RP), nrow = ncol(RP))
cols = c("white", "blue4")
image(RP, xlab = "", ylab = "", col = cols)

#Tags of coordination(?)
Profile = drpdfromts(pitch1$f0, pitch1$f0, datatype = 'continuous', ws=50, radius = ans$radius)

timecourse = round(seq(-5000,5000,100)/1000, digit = 1)

maxlag = Profile$maxlag/1000
profile = Profile$profile*100
Prof = data.frame(profile)
ggplot(Prof, aes(timecourse, profile))+geom_line()+geom_vline(xintercept = timecourse[maxlag], color = 'red')
```



2. Second you will have to turn the code into a function and loop through all the files (or even better use apply/sapply/lapply)
- Remember to extract the relevant information from the file names (Participant, Diagnosis, Trial, Study)

```{r}
#creating a variable to store the filenames
files = list.files(path = "Pitch/Pitch/")
#saving the folder name
folder = "Pitch/Pitch/"

#defining a function to extract optimal parameters - RUN FIRST!!!!!!!!!!!!
opt_par_extractor = function(filename){ 
  #reading the file
  f = read.delim(paste(folder, filename, sep = "")) 
  #setting parameters (Riccardo's code)
  par = list(lgM =  50, steps = seq(1, 6, 1),  radiusspan = 100,  radiussample = 40, normalize = 0,  rescale = 0,  mindiagline = 2,  minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE,  fnnpercent = 10,  typeami = "mindip")
  #has to be set to NULL first, otherwise it won't work if there's more than one trial
  opt_param = NULL
  #getting the optimal parameters and saving to opt_param. Using try so it doesn't crash if there is an error
  opt_param = try(optimizeParam(f$f0, f$f0, par, min.rec = 3.5, max.rec = 4.5))
  #if the optimize function succeeded, save the results to a dataframe - if not, put NAs 
  if (length(opt_param) > 1) {
    result_df = data.frame(radius = opt_param[1], emddim = opt_param[2], delay = opt_param[3], filename = filename) 
    } else {
    result_df = data.frame(radius = NA, emddim = NA, delay = NA, filename = filename) 
    }
  return(result_df)
}

##using lapply to apply the function on all the files
opt_df = lapply(files, opt_par_extractor)
#checking how it works
head(opt_df)
#turning it into a dataframe
opt_df = bind_rows(opt_df)
#adding columns with the optimal parameters (i choose the median value for all of them)
opt_df = opt_df %>% mutate(opt_dim = median(opt_df$emddim, na.rm = T), opt_delay = median(opt_df$delay, na.rm = T), opt_rad = median(opt_df$radius, na.rm = T))

#writing parameters to file
#write.csv(opt_df, "opt_params.csv") #RUN ONLY ONCE

#defining a function to only extract the different RQA values - YOU DON'T NEED TO RUN THIS
rqa_extractor = function(filename){ #making a function which applies the optimal parameters and then saves the rqa results
  f = read.delim(paste(folder, filename, sep = ""))
  result = try(crqa(f$f0, f$f0, embed = opt_df$opt_dim, delay = opt_df$opt_delay, radius = opt_df$opt_rad, normalize = 0,  rescale = 0,  mindiagline = 2,  minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE))
  if (length(result) > 1){
    results_df = data.frame(RR = result[1], DET = result[2], NRLINE = result[3], 
               maxL = result[4], L = result[5], ENTR = result[6],
               rENTR = result[7], LAM = result[8], TT = result[9], filename = filename)
    #RR = % black dots (also called REC), DET = how likely is it the next black dot is black (number_of_recurrences/total_observations), L = The average length of line structures, maxL = the longest the diagonal line (also called MDL), ENTR = entrophy, TT = average length of vertical lines
  } else {
    results_df = data.frame(RR = NA, DET = NA, NRLINE = NA, 
               maxL = NA, L = NA, ENTR = NA,
               rENTR = NA, LAM = NA, TT = NA, filename = filename)    
  }
  
  return(results_df)
}


#Function to calculate 'conventional' statistical measures - YOU DON'T NEED TO RUN THIS
descrip_stats = function(filename) {
  f = read.delim(paste(folder, filename, sep = ""))
  meanv = mean(f$f0, rm.na = T)
  sdv = sd(f$f0)
  medianv = median(f$f0)
  rangev = max(f$f0) - min(f$f0)
  iqrv = IQR(f$f0)
  madv = aad(f$f0)
  coefvarv = sd(f$f0)/mean(f$f0)
  Diagnosis = str_extract(filename, "D\\d+")
  Time = str_extract(filename, "T\\d+")
  participant = str_extract(filename, "\\d{3}")
  results_df = data.frame(mean = meanv, sd = sdv, median = medianv, range = rangev, iqr = iqrv, mad = madv, coefvar = coefvarv, participantID = participant, time = Time, diagnosis = Diagnosis, filename = filename)
  return(results_df)
}



#function to extract rqa values (based on the opt_par_df) as well as more conventional stats - USE THIS!
comb_fun = function(filename) {
  #reading data 
  f = read.delim(paste(folder, filename, sep = ""))
  #calculating conventional statistical measures
  meanv = mean(f$f0, rm.na = T)
  sdv = sd(f$f0)
  medianv = median(f$f0)
  rangev = max(f$f0) - min(f$f0)
  iqrv = IQR(f$f0)
  madv = aad(f$f0)
  coefvarv = sd(f$f0)/mean(f$f0)
  Diagnosis = str_extract(filename, "D\\d+")
  Time = str_extract(filename, "T\\d+")
  participant = str_extract(filename, "\\d{3}")
  #calculating and extracting RQA values
  result = try(crqa(f$f0, f$f0, embed = opt_df$opt_dim, delay = opt_df$opt_delay, radius = opt_df$opt_rad, normalize = 0,  rescale = 0,  mindiagline = 2,  minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE))
  if (length(result) > 1){
    results_df = data.frame(mean = meanv, sd = sdv, median = medianv, range = rangev, 
                            iqr = iqrv, mad = madv, coefvar = coefvarv, participantID = participant, 
                            time = Time, diagnosis = Diagnosis, RR = result[1], DET = result[2], 
                            NRLINE = result[3], maxL = result[4], L = result[5], ENTR = result[6], 
                            rENTR = result[7], LAM = result[8], TT = result[9], filename =  filename)
    #RR = % black dots (also called REC), DET = how likely is it the next black dot is black (number_of_recurrences/total_observations), L = The average length of line structures, maxL = the longest the diagonal line (also called MDL), ENTR = entrophy, TT = average length of vertical lines
  } else {
    results_df = data.frame(mean = meanv, sd = sdv, median = medianv, range = rangev, 
                            iqr = iqrv, mad = madv, coefvar = coefvarv, participantID = participant, 
                            time = Time, diagnosis = Diagnosis, RR = NA, DET = NA, NRLINE = NA, 
                            maxL = NA, L = NA, ENTR = NA, rENTR = NA, LAM = NA, TT = NA, filename = filename)    
  }
  
  return(results_df)  
}
#reading the file of optimalparameters - the function needs this to work
opt_df = read.csv("opt_params.csv")
#using the big function on all the files
comb_df = lapply(files, comb_fun)
#making it into a dataframe
comb_df = bind_rows(comb_df)

#writing combined dataset to csv
#write.csv(comb_df, "schizoAcoustics.csv")
head(comb_df)
```


3. Make one model per acoustic feature and test whether you can observe significant difference due to Diagnosis. Tip: Which other fixed factors should you control for (that is, include in the model)? Which random ones?
- Bonus points: cross-validate the model and report the betas and standard errors from all rounds to get an idea of how robust the estimates are. 

```{r}
###MERGING ALL THE DATASETS

df = read.csv("schizoAcoustics.csv")
#Extracting study from filename
df = df %>% mutate(Study = str_extract(filename, "S\\d"))

#removing characters from study, and making it a factor
df$Study = str_extract(df$Study, "\\d")
df$Study = as.factor(df$Study)
levels(df$Study)

#making ID a factor
df$participantID = as.factor(df$participantID)

#renaming factors of diagnosis assuming D0 = Control and D1 = Schizophrenia
df$diagnosis = recode(df$diagnosis,  D0 = "Control", D1 = "Schizophrenia")

#removing characters from time making it a factor
df$time = as.character(df$time)
df$time = str_extract(df$time, "\\d+")
df$time = as.factor(df$time)
levels(df$time)
#re-ordering the factor levels of time
df$time = factor(df$time,levels(df$time)[c(1, 3, 4, 5, 6, 7, 8, 9, 10, 2)])

####MERGE WITH DEMO AND ARTICULATION DATA
names(df)[names(df) == 'participantID'] <- 'Subject'
names(df)[names(df) == 'study'] <- 'Study'
names(df)[names(df) == 'diagnosis'] <- 'Diagnosis'
levels(df$Diagnosis)
demo$Study = as.factor(demo$Study)
demo$Subject = as.factor(demo$Subject)


#merging demographic and RQA data
demorqa = merge(df, demo, by = c("Subject", "Study", "Diagnosis"))
#adding articulation data
names(arti)[names(arti) == 'soundname'] <- 'filename'
demorqa$filename = gsub("_f0.txt", "", demorqa$filename)
demorqa$filename = as.factor(demorqa$filename)

all_df = merge(demorqa, arti, by = "filename" )
#writing the full df to csv
#write.csv(all_df, "combinedSchizoData.csv")

```

```{r}
#Making a model for each acoustic feature
df = read.csv("combinedSchizoData.csv")
#woops, time is time but actually which video they saw
names(df)[names(df)=="time"] <- "video"
df$Study = as.factor(df$Study)

mDET = lmer(DET ~ Diagnosis * Study + (1+video|Subject), df)
mRR = lmer(RR ~ Diagnosis + (1+video|Subject), df)
mNRLINE = lmer(NRLINE~ Diagnosis + (1+video|Subject), df)
mmaxL = lmer(maxL ~ Diagnosis + (1+video|Subject), df)
mL = lmer(L ~ Diagnosis + (1+video|Subject), df)
mENTR = lmer(ENTR ~ Diagnosis + (1+video|Subject), df)
mrENTR = lmer(rENTR ~ Diagnosis + (1+video|Subject), df) #model doesn't converge :(
mLAM = lmer(LAM ~ Diagnosis + (1+video|Subject), df)
mTT = lmer(TT ~ Diagnosis + (1+video|Subject), df)
mMean = lmer(mean ~ Diagnosis + Study + (1+video|Subject), df)

summary(mDET)
summary(mRR)
summary(mNRLINE)
summary(mmaxL)
summary(mL)
summary(mENTR)
summary(mrENTR)
summary(mLAM)
summary(mTT)
summary(mMean)
```

3a. Is study a significant predictor in these models? What should you infer from this? Does study interact with diagnosis? What should you infer from this?

4. Bonus Question: Compare effect size of diagnosis across the different measures. Which measure seems most sensitive?
- Tip: to compare across measures you need to put all of them on the same scale, that is, you need to "standardize" them (z-score)

5. Bonus question. In the Clinical Info file you have additional information about the participants. Which additional parameters (e.g. age, gender) should we control for? Report the effects.

6. Write a paragraph reporting methods and results

[Next assignment: can we use these measures to build a tool that diagnoses people from voice only?]

## N.B. Remember to save the acoustic features of voice in a separate file, so to be able to load them next time